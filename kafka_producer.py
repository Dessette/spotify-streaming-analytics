import pandas as pd
import json
import time
from confluent_kafka import Producer
from datetime import datetime, timedelta
import os
import random
import logging

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SpotifyDataStreamer:
    def __init__(self, kafka_bootstrap_servers='localhost:9092'):
        """
        Spotify verilerini Kafka'ya stream eden producer (Confluent Kafka)
        """
        # Confluent Kafka Producer config
        self.producer_config = {
            'bootstrap.servers': kafka_bootstrap_servers,
            'client.id': 'spotify-producer',
            'compression.type': 'gzip',
            'batch.size': 16384,
            'linger.ms': 10,
            'retries': 5,
            'acks': 'all'
        }
        
        self.producer = Producer(self.producer_config)
        
        # Kafka topics
        self.topics = {
            'historical_stream': 'spotify-historical-stream',
            'real_time_events': 'spotify-realtime-events',
            'analytics_results': 'spotify-analytics-results'
        }
        
        self.data = None
        self.load_data()
        
    def delivery_report(self, err, msg):
        """Kafka delivery callback"""
        if err is not None:
            logger.error(f'Message delivery failed: {err}')
        else:
            logger.debug(f'Message delivered to {msg.topic()} [{msg.partition()}]')
        
    def load_data(self):
        """Spotify dataset'ini ./data klasÃ¶rÃ¼nden yÃ¼kle"""
        data_dir = './data'
        
        logger.info(f"ğŸ“ Dataset aranÄ±yor: {data_dir}")
        
        if not os.path.exists(data_dir):
            raise FileNotFoundError(
                f"âŒ '{data_dir}' klasÃ¶rÃ¼ bulunamadÄ±!\n"
                f"ğŸ› ï¸ Ã‡Ã¶zÃ¼m:\n"
                f"   1. mkdir data\n"
                f"   2. Kaggle'dan dataset'i indir: https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset\n"
                f"   3. ZIP dosyasÄ±nÄ± '{data_dir}' klasÃ¶rÃ¼ne Ã§Ä±kart"
            )
        
        # CSV dosyalarÄ±nÄ± bul
        csv_files = []
        for file in os.listdir(data_dir):
            if file.endswith('.csv'):
                csv_files.append(os.path.join(data_dir, file))
        
        if not csv_files:
            raise FileNotFoundError(
                f"âŒ '{data_dir}' klasÃ¶rÃ¼nde CSV dosyasÄ± bulunamadÄ±!\n"
                f"ğŸ› ï¸ Ã‡Ã¶zÃ¼m:\n"
                f"   1. https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset\n"
                f"   2. 'Download' butonuna tÄ±kla\n"
                f"   3. ZIP dosyasÄ±nÄ± '{data_dir}' klasÃ¶rÃ¼ne Ã§Ä±kart\n"
                f"   4. CSV dosyasÄ±nÄ±n doÄŸru yerde olduÄŸunu kontrol et"
            )
        
        # Ä°lk CSV dosyasÄ±nÄ± yÃ¼kle
        csv_path = csv_files[0]
        logger.info(f"ğŸ“„ CSV dosyasÄ± bulundu: {csv_path}")
        
        try:
            self.data = pd.read_csv(csv_path)
            logger.info(f"âœ… Dataset baÅŸarÄ±yla yÃ¼klendi: {len(self.data)} kayÄ±t, {len(self.data.columns)} kolon")
            
            # Veriyi temizle ve hazÄ±rla
            self.prepare_data()
            
        except Exception as e:
            raise Exception(f"âŒ CSV dosyasÄ± okunamadÄ±: {e}\n"
                          f"ğŸ› ï¸ Dosya bozuk olabilir, tekrar indirmeyi dene")
    

    
    def prepare_data(self):
        """Veriyi temizle ve hazÄ±rla"""
        try:
            # Kaggle dataset'inin gerÃ§ek column isimlerini standart isimlere Ã§evir
            column_mapping = {
                'Index': 'index',
                'Title': 'title',
                'Artist': 'artist',
                'Top Genre': 'top_genre',
                'Year': 'year',
                'Beats Per Minute (BPM)': 'bpm',
                'Energy': 'energy',
                'Danceability': 'danceability',
                'Loudness (dB)': 'loudness',
                'Liveness': 'liveness',
                'Valence': 'valence',
                'Length (Duration)': 'length',
                'Acousticness': 'acousticness',
                'Speechiness': 'speechiness',
                'Popularity': 'popularity'
            }
            
            # Column isimlerini yeniden adlandÄ±r
            self.data = self.data.rename(columns=column_mapping)
            logger.info(f"ğŸ“‹ Column isimleri standartlaÅŸtÄ±rÄ±ldÄ±")
            
            # Eksik verileri temizle
            self.data = self.data.dropna()
            
            # Veri tiplerini kontrol et ve dÃ¼zelt
            numeric_columns = ['year', 'bpm', 'energy', 'danceability', 'loudness', 
                             'liveness', 'valence', 'length', 'acousticness', 'speechiness', 'popularity']
            
            for col in numeric_columns:
                if col in self.data.columns:
                    self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
            
            # String kolonlarÄ± temizle
            string_columns = ['title', 'artist', 'top_genre']
            for col in string_columns:
                if col in self.data.columns:
                    self.data[col] = self.data[col].astype(str).str.strip()
            
            # Veriyi yÄ±la gÃ¶re sÄ±rala (historical replay iÃ§in)
            if 'year' in self.data.columns:
                self.data = self.data.sort_values('year').reset_index(drop=True)
            
            logger.info(f"ğŸ“Š Data hazÄ±rlandÄ±: {len(self.data)} kayÄ±t, {len(self.data.columns)} kolon")
            logger.info(f"ğŸµ YÄ±l aralÄ±ÄŸÄ±: {self.data['year'].min()} - {self.data['year'].max()}")
            logger.info(f"ğŸ¤ Benzersiz sanatÃ§Ä± sayÄ±sÄ±: {self.data['artist'].nunique()}")
            logger.info(f"ğŸ¸ Benzersiz genre sayÄ±sÄ±: {self.data['top_genre'].nunique()}")
            
        except Exception as e:
            logger.error(f"Veri hazÄ±rlama hatasÄ±: {e}")
            raise
    
    def create_song_event(self, row, event_type='historical'):
        """ÅarkÄ± verisini Kafka event'ine dÃ¶nÃ¼ÅŸtÃ¼r"""
        # NaN deÄŸerleri gÃ¼venli ÅŸekilde integer'a Ã§evir
        def safe_int(value, default=0):
            try:
                if pd.isna(value) or value == '' or value is None:
                    return default
                return int(float(value))
            except (ValueError, TypeError):
                return default
        
        def safe_str(value, default='Unknown'):
            try:
                if pd.isna(value) or value == '' or value is None:
                    return default
                return str(value).strip()
            except:
                return default
        
        return {
            'event_type': event_type,
            'timestamp': datetime.now().isoformat(),
            'song_data': {
                'title': safe_str(row['title']),
                'artist': safe_str(row['artist']),
                'top_genre': safe_str(row['top_genre']),
                'year': safe_int(row['year'], 2000),
                'bpm': safe_int(row['bpm'], 120),
                'energy': safe_int(row['energy'], 50),
                'danceability': safe_int(row['danceability'], 50),
                'loudness': safe_int(row['loudness'], -10),
                'liveness': safe_int(row['liveness'], 10),
                'valence': safe_int(row['valence'], 50),
                'length': safe_int(row['length'], 180000),
                'acousticness': safe_int(row['acousticness'], 20),
                'speechiness': safe_int(row['speechiness'], 5),
                'popularity': safe_int(row['popularity'], 50)
            },
            'metadata': {
                'processing_time': datetime.now().isoformat(),
                'source': 'spotify_data'
            }
        }
    
    def send_message(self, topic, key, value):
        """Confluent Kafka ile mesaj gÃ¶nder"""
        try:
            self.producer.produce(
                topic=topic,
                key=str(key) if key else None,
                value=json.dumps(value, ensure_ascii=False).encode('utf-8'),
                callback=self.delivery_report
            )
        except Exception as e:
            logger.error(f"Mesaj gÃ¶nderme hatasÄ±: {e}")
    
    def simulate_real_time_events(self):
        """Rastgele real-time events Ã¼ret"""
        random_song = self.data.sample(1).iloc[0]
        
        popularity_boost = random.randint(15, 40)
        boosted_song = random_song.copy()
        boosted_song['popularity'] = min(100, boosted_song['popularity'] + popularity_boost)
        
        event = self.create_song_event(boosted_song, 'popularity_surge')
        event['metadata']['boost_amount'] = popularity_boost
        event['metadata']['reason'] = random.choice([
            'viral_tiktok', 'playlist_feature', 'artist_collab', 'radio_play', 'meme_trend', 'influencer_post'
        ])
        event['metadata']['original_popularity'] = int(random_song['popularity'])
        event['metadata']['new_popularity'] = int(boosted_song['popularity'])
        
        return event
    
    def stream_historical_data(self, seconds_per_year=10, batch_size=10):
        """
        Spotify verisini yÄ±llara gÃ¶re zamanlÄ± olarak Kafka'ya gÃ¶nderir.
        Ã–rneÄŸin: seconds_per_year=10 â†’ Her yÄ±lÄ±n ÅŸarkÄ±larÄ±nÄ± 10 saniyede gÃ¶nder.
        """
        logger.info("ğŸµ Historical data streaming baÅŸlÄ±yor...")

        total_songs = len(self.data)
        processed = 0

        yearly_data = self.data.groupby('year')

        for year, year_df in yearly_data:
            logger.info(f"ğŸ“… {year} yÄ±lÄ±: {len(year_df)} ÅŸarkÄ± hazÄ±rlanÄ±yor...")

            start_time = time.time()
            year_df = year_df.reset_index(drop=True)

            for i in range(0, len(year_df), batch_size):
                batch = year_df.iloc[i:i+batch_size]

                for _, song in batch.iterrows():
                    try:
                        event = self.create_song_event(song, 'historical')
                        self.send_message(
                            topic=self.topics['historical_stream'],
                            key=f"{song['artist']}_{song['title']}",
                            value=event
                        )
                        processed += 1

                        # Durum gÃ¼ncelleme
                        if processed % 200 == 0:
                            progress = (processed / total_songs) * 100
                            logger.info(f"âš¡ Ä°lerleme: {processed}/{total_songs} ({progress:.1f}%) - Son: {song['artist']} - {song['title']}")

                    except Exception as e:
                        logger.error(f"âŒ ÅarkÄ± gÃ¶nderme hatasÄ±: {e}")

                self.producer.poll(0)  # buffer boÅŸalt

            # GerÃ§ek zamanlÄ± event ekle (%5 olasÄ±lÄ±k)
            if random.random() < 0.05:
                try:
                    rt_event = self.simulate_real_time_events()
                    self.send_message(
                        topic=self.topics['real_time_events'],
                        key=None,
                        value=rt_event
                    )
                    song_title = rt_event['song_data']['title']
                    artist = rt_event['song_data']['artist']
                    reason = rt_event['metadata']['reason']
                    old_pop = rt_event['metadata']['original_popularity']
                    new_pop = rt_event['metadata']['new_popularity']
                    logger.info(f"ğŸ”¥ Real-time event: '{song_title}' by {artist} - {reason} ({old_pop}â†’{new_pop})")
                except Exception as e:
                    logger.error(f"âŒ Real-time event hatasÄ±: {e}")

            # YÄ±l baÅŸÄ±na sÃ¼reyi dengele
            elapsed = time.time() - start_time
            remaining = max(0, seconds_per_year - elapsed)
            if remaining > 0:
                logger.info(f"â¸ï¸ {year} yÄ±lÄ± tamamlandÄ±, {remaining:.2f} saniye bekleniyor...")
                time.sleep(remaining)

        self.producer.flush()
        logger.info(f"âœ… Streaming tamamlandÄ±! Toplam {processed} ÅŸarkÄ± gÃ¶nderildi.")
        logger.info(f"ğŸ“ˆ Kafka topics'e baÅŸarÄ±yla stream edildi:")
        logger.info(f"   â€¢ {self.topics['historical_stream']}: {processed} historical events")
        logger.info(f"   â€¢ {self.topics['real_time_events']}: ~{processed//20} real-time events")

    
    def close(self):
        """Producer'Ä± kapat"""
        self.producer.flush()

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    # Ã–nce data klasÃ¶rÃ¼nÃ¼ kontrol et
    if not os.path.exists('./data'):
        print("âŒ './data' klasÃ¶rÃ¼ bulunamadÄ±!")
        print("\nğŸ› ï¸ DATASET KURULUM REHBERÄ°:")
        print("1. mkdir data")
        print("2. https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset adresine git")
        print("3. 'Download' butonuna tÄ±kla")
        print("4. ZIP dosyasÄ±nÄ± './data/' klasÃ¶rÃ¼ne Ã§Ä±kart")
        print("5. Producer'Ä± tekrar Ã§alÄ±ÅŸtÄ±r")
        exit(1)
    
    try:
        print("ğŸµ" + "="*60 + "ğŸµ")
        print("    ğŸ§ SPOTIFY REAL-TIME ANALYTICS PRODUCER ğŸ§    ")
        print("ğŸµ" + "="*60 + "ğŸµ")
        print()
        
        # Streamer oluÅŸtur
        logger.info("ğŸš€ Spotify Data Streamer baÅŸlatÄ±lÄ±yor...")
        streamer = SpotifyDataStreamer()
        
        print(f"âœ… Dataset yÃ¼klendi ve hazÄ±rlandÄ±!")
        print(f"ğŸ“Š Toplam ÅŸarkÄ±: {len(streamer.data)}")
        print(f"ğŸ¯ YÄ±l aralÄ±ÄŸÄ±: {streamer.data['year'].min()} - {streamer.data['year'].max()}")
        print(f"ğŸ¤ SanatÃ§Ä± sayÄ±sÄ±: {streamer.data['artist'].nunique()}")
        print(f"ğŸ¸ Genre sayÄ±sÄ±: {streamer.data['top_genre'].nunique()}")
        print()
        
        # Historical Data Replay'i baÅŸlat
        print("âš¡ Historical Data Replay baÅŸlatÄ±lÄ±yor...")
        print("ğŸ“ˆ 2000'li yÄ±llarÄ±n mÃ¼zik verisi zamanlÄ± olarak stream ediliyor...")
        print("ğŸ”„ Time compression: 1000x (1 yÄ±l = ~1 saniye)")
        print("ğŸ’¡ Real-time event'ler rastgele enjekte ediliyor...")
        print()
        print("â¹ï¸  Durdurmak iÃ§in Ctrl+C'ye basÄ±n")
        print("-" * 60)
        
        # Streaming'i baÅŸlat
        streamer.stream_historical_data(seconds_per_year=5, batch_size=20)
        
    except Exception as e:
        logger.error(f"Ana hata: {e}")
        print(f"\n{e}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Streaming durduruldu.")
    finally:
        try:
            streamer.close()
        except:
            pass
        print("ğŸ‘‹ Producer kapatÄ±ldÄ±.")